# Zen MCP Server - 多模型协调指挥系统

> BeehiveInnovations开发的MCP服务器，让Claude作为指挥员协调调用o3、gemini2.5pro、flash等模型，发挥各自优势

## 目录导航

1. [什么是 Zen MCP Server？](#1-什么是-zen-mcp-server)
2. [核心功能特性](#2-核心功能特性)
3. [系统架构](#3-系统架构)
4. [安装指南](#4-安装指南)
5. [配置说明](#5-配置说明)
6. [使用方法](#6-使用方法)
7. [协调策略](#7-协调策略)
8. [故障排除](#8-故障排除)

## 1. 什么是 Zen MCP Server？

Zen MCP Server 是由 BeehiveInnovations 开发的创新性 Model Context Protocol (MCP) 服务器，它将 Claude Code 转变为一个智能指挥官，能够协调调用多个不同的 AI 模型（如 o3、Gemini 2.5 Pro、Flash 等），充分发挥各个模型的优势，实现更强大和高效的 AI 协作系统。

### 核心价值

- **多模型协调**：让 Claude 作为指挥官协调多个 AI 模型
- **优势互补**：发挥不同模型的独特优势和专长
- **智能编排**：根据任务特点智能选择最适合的模型组合
- **透明集成**：对用户透明的多模型协作体验
- **性能优化**：通过模型组合实现最佳性能和成本效益

### 设计理念

Zen MCP Server 遵循现代 AI 系统协作的前沿理念：
- **专业化分工**：不同模型专注各自擅长的任务
- **智能协调**：AI 模型间的智能协调和通信
- **透明体验**：用户无需了解底层模型选择逻辑
- **持续学习**：从使用中学习优化协调策略
- **可扩展性**：支持添加新的 AI 模型和服务

## 2. 核心功能特性

### 🎯 智能指挥官模式
- **Claude 指挥**：Claude 作为主指挥官协调其他模型
- **任务分析**：智能分析任务需求和复杂度
- **模型选择**：根据任务特点选择最优模型组合
- **协作编排**：协调多模型协同完成任务
- **结果整合**：整合多个模型的输出结果

### 🤖 多模型支持
- **OpenAI o3**：最新的 OpenAI 模型，用于复杂推理
- **Gemini 2.5 Pro**：Google 的先进多模态模型
- **Claude 系列**：Anthropic 的 Claude 模型家族
- **Flash 模型**：快速响应的轻量级模型
- **自定义模型**：支持集成自定义 AI 模型

### 🔄 任务分解与分配
- **智能分解**：将复杂任务智能分解为子任务
- **能力映射**：将子任务映射到最适合的模型
- **并行处理**：支持多个模型并行处理不同子任务
- **依赖管理**：管理子任务间的依赖关系
- **结果整合**：智能整合和协调各模型的结果

### 📊 性能优化
- **成本优化**：根据任务需求选择最具成本效益的模型
- **速度优化**：平衡速度和质量，优化响应时间
- **质量保证**：确保最终输出质量符合要求
- **资源管理**：有效管理和分配计算资源
- **负载均衡**：在多个模型间平衡工作负载

### 🔧 配置和管理
- **灵活配置**：支持多种配置方式和参数调整
- **实时监控**：实时监控各模型的状态和性能
- **动态调整**：根据使用情况动态调整配置
- **版本管理**：支持模型版本管理和升级
- **安全控制**：提供完善的安全和权限控制

### 🌐 扩展性
- **插件架构**：支持插件式扩展新功能
- **API 接口**：提供丰富的 API 接口
- **协议支持**：支持多种 AI 模型 API 协议
- **自定义策略**：支持自定义协调策略
- **社区生态**：活跃的社区和插件生态

## 3. 系统架构

### 技术架构
```
User (Claude Code)
    ↓
Zen MCP Server
    ├── Command Interpreter
    ├── Task Analyzer
    ├── Model Orchestrator
    ├── Result Aggregator
    └── Performance Monitor
    ↓
Multiple AI Models
    ├── OpenAI o3
    ├── Gemini 2.5 Pro
    ├── Claude Models
    ├── Flash Models
    └── Custom Models
```

### 核心组件

#### Command Interpreter（命令解释器）
- **命令解析**：解析和验证用户输入的命令
- **意图识别**：识别用户的真实意图和需求
- **参数提取**：提取命令中的关键参数
- **上下文理解**：理解命令的上下文环境
- **安全检查**：执行安全性和权限检查

#### Task Analyzer（任务分析器）
- **复杂度评估**：评估任务的复杂度和难度
- **需求分析**：分析任务的具体需求
- **可行性评估**：评估任务的可完成性
- **资源评估**：评估所需的资源和时间
- **风险识别**：识别潜在的风险和挑战

#### Model Orchestrator（模型编排器）
- **模型选择**：根据任务特点选择最适合的模型
- **任务分配**：将子任务分配给相应的模型
- **协调管理**：协调多个模型的协作过程
- **状态监控**：监控各模型的执行状态
- **异常处理**：处理执行过程中的异常情况

#### Result Aggregator（结果聚合器）
- **结果收集**：收集各模型的执行结果
- **质量评估**：评估结果的质量和准确性
- **冲突解决**：解决不同模型结果间的冲突
- **结果整合**：将多个结果整合成最终输出
- **格式化输出**：按用户需求格式化最终结果

## 4. 安装指南

### 4.1 系统要求

#### 最低要求
- **操作系统**：Linux (Ubuntu 18.04+), macOS 10.15+, Windows 10+
- **Claude Code**：最新版本
- **Node.js**：18.0.0+ 或 Python 3.9+
- **内存**：8GB RAM
- **存储空间**：5GB 可用空间
- **网络**：稳定的互联网连接

#### 推荐配置
- **操作系统**：Linux (Ubuntu 20.04+), macOS 12+, Windows 11+
- **Claude Code**：最新版本
- **Node.js**：20.0.0+ 或 Python 3.11+
- **内存**：16GB RAM 或更多
- **存储空间**：10GB 可用空间
- **网络**：高速宽带连接

#### API 密钥要求
- **Anthropic API Key**：用于 Claude 模型
- **OpenAI API Key**：用于 o3 和其他 OpenAI 模型
- **Google AI API Key**：用于 Gemini 模型
- **其他模型 API Key**：根据使用的模型而定

### 4.2 安装方式

#### 方式1：NPM 安装
```bash
# 安装 zen-mcp-server
npm install -g @beehiveinnovations/zen-mcp-server

# 验证安装
zen-mcp-server --version

# 初始化配置
zen-mcp-server init
```

#### 方式2：Git 克隆安装
```bash
# 克隆仓库
git clone https://github.com/BeehiveInnovations/zen-mcp-server.git
cd zen-mcp-server

# 安装依赖
npm install

# 构建项目
npm run build

# 全局链接
npm link

# 验证安装
zen-mcp-server --version
```

#### 方式3：Python 安装
```bash
# 使用 pip 安装
pip install zen-mcp-server

# 验证安装
zen-mcp-server --version

# 初始化配置
zen-mcp-server init
```

#### 方式4：Docker 部署
```bash
# 拉取镜像
docker pull beehiveinnovations/zen-mcp-server

# 运行容器
docker run -d \
  --name zen-mcp-server \
  -p 8080:8080 \
  -v $(pwd)/config:/app/config \
  -e OPENAI_API_KEY=your-key \
  -e ANTHROPIC_API_KEY=your-key \
  -e GOOGLE_API_KEY=your-key \
  beehiveinnovations/zen-mcp-server
```

### 4.3 Claude Code 集成

#### 配置 MCP 服务器
```json
// ~/.claude/settings.json
{
  "mcpServers": {
    "zen-mcp": {
      "command": "zen-mcp-server",
      "args": ["--mcp-server"],
      "env": {
        "OPENAI_API_KEY": "your-openai-key",
        "ANTHROPIC_API_KEY": "your-anthropic-key",
        "GOOGLE_API_KEY": "your-google-key",
        "ZEN_CONFIG_PATH": "/path/to/zen-config.yaml"
      }
    }
  }
}
```

#### 验证集成
```bash
# 在 Claude Code 中测试
/ask "zen-mcp 状态"

# 查看可用模型
/ask "列出所有可用的AI模型"

# 测试多模型协作
/ask "使用zen-mcp协调多个模型分析这个问题"
```

## 5. 配置说明

### 5.1 基础配置

#### 主配置文件
```yaml
# zen-config.yaml
server:
  host: "localhost"
  port: 8080
  workers: 4

orchestrator:
  primary_model: "claude"  # 指挥官模型
  max_parallel_tasks: 5
  timeout: 300
  retry_attempts: 3

models:
  claude:
    provider: "anthropic"
    model: "claude-3-opus-20240229"
    role: "commander"
    capabilities: ["reasoning", "coordination", "analysis"]
    api_key: "${ANTHROPIC_API_KEY}"

  openai_o3:
    provider: "openai"
    model: "o3-preview"
    role: "specialist"
    capabilities: ["math", "logic", "complex_reasoning"]
    api_key: "${OPENAI_API_KEY}"

  gemini_pro:
    provider: "google"
    model: "gemini-2.0-flash-exp"
    role: "analyst"
    capabilities: ["multimodal", "analysis", "creative"]
    api_key: "${GOOGLE_API_KEY}"

  flash:
    provider: "openai"
    model: "gpt-3.5-turbo"
    role: "fast_responder"
    capabilities: ["quick_response", "simple_tasks"]
    api_key: "${OPENAI_API_KEY}"
```

#### 任务策略配置
```yaml
strategies:
  coding_tasks:
    primary_model: "claude"
    specialists: ["openai_o3"]
    workflow: "analyze -> design -> implement -> review"
    cost_optimization: true

  analysis_tasks:
    primary_model: "claude"
    specialists: ["gemini_pro"]
    parallel_processing: true
    quality_threshold: 0.8

  creative_tasks:
    primary_model: "gemini_pro"
    specialists: ["claude"]
    brainstorming_enabled: true
    iteration_limit: 3

  simple_tasks:
    primary_model: "flash"
    fallback_model: "claude"
    timeout: 30
```

### 5.2 高级配置

#### 协调策略配置
```yaml
orchestration:
  decision_engine:
    type: "hybrid"  # rule_based, ml_based, hybrid
    confidence_threshold: 0.7
    learning_enabled: true

  load_balancing:
    strategy: "adaptive"
    weights:
      claude: 3
      openai_o3: 2
      gemini_pro: 2
      flash: 1

  failover:
    enabled: true
    backup_models: ["claude"]
    recovery_timeout: 60
```

#### 成本优化配置
```yaml
cost_optimization:
  enabled: true
  budget_limits:
    daily: 100.0
    monthly: 2000.0

  model_priorities:
    low_cost_tasks: ["flash", "gemini_pro"]
    medium_cost_tasks: ["claude"]
    high_cost_tasks: ["openai_o3"]

  cost_tracking:
    enabled: true
    real_time_alerts: true
    report_frequency: "daily"
```

#### 性能优化配置
```yaml
performance:
  caching:
    enabled: true
    ttl: 3600
    max_size: 1000

  connection_pooling:
    enabled: true
    max_connections: 20
    idle_timeout: 300

  request_timeout:
    default: 60
    per_model:
      claude: 45
      openai_o3: 90
      gemini_pro: 60
      flash: 30
```

## 6. 使用方法

### 6.1 基础使用

#### 启动服务器
```bash
# 使用默认配置启动
zen-mcp-server

# 使用指定配置启动
zen-mcp-server --config /path/to/zen-config.yaml

# 启动调试模式
zen-mcp-server --debug

# 启动Web界面
zen-mcp-server --web-ui --port 8080
```

#### 与 Claude Code 交互
```bash
# 在 Claude Code 中使用命令
/zen-status  # 查看服务器状态
/zen-models  # 列出可用模型
/zen-stats   # 查看使用统计
```

### 6.2 协调命令

#### 指定模型协作
```bash
# 让 Claude 协调多个模型
"请使用 zen-mcp 协调 o3 和 Gemini 分析这个算法"

# 指定特定的模型组合
"使用 Claude 作为指挥官，让 o3 处理数学部分，Gemini 处理可视化部分"

# 并行处理不同任务
"让 flash 快速生成基础代码，然后让 o3 优化算法复杂度"
```

#### 任务类型指定
```bash
# 编程任务
"这是一个编程任务，请使用最合适的模型组合来解决"

# 分析任务
"请使用多个模型分析这个数据集，提供不同的视角"

# 创意任务
"请发挥创造性，使用擅长创意的模型来设计方案"
```

### 6.3 高级功能

#### 自定义工作流
```yaml
# custom-workflows.yaml
workflows:
  code_review:
    steps:
      - model: "claude"
        task: "initial_code_review"
      - model: "openai_o3"
        task: "algorithm_analysis"
      - model: "gemini_pro"
        task: "security_analysis"
      - model: "claude"
        task: "final_synthesis"

  research_analysis:
    steps:
      - model: "gemini_pro"
        task: "data_collection"
      - model: "claude"
        task: "pattern_analysis"
      - model: "openai_o3"
        task: "statistical_analysis"
```

#### 执行自定义工作流
```bash
# 执行代码审查工作流
/zen-execute-workflow code_review --input "src/main.py"

# 执行研究分析工作流
/zen-execute-workflow research_analysis --input "dataset.csv"
```

### 6.4 监控和管理

#### 查看实时状态
```bash
# 查看服务器状态
zen-mcp-server status

# 查看模型状态
zen-mcp-server models status

# 查看任务执行情况
zen-mcp-server tasks list
```

#### 性能监控
```bash
# 查看性能指标
zen-mcp-server performance metrics

# 查看使用统计
zen-mcp-server stats usage

# 生成性能报告
zen-mcp-server report performance --last 24h
```

## 7. 协调策略

### 7.1 智能决策算法

#### 任务-模型映射策略
```yaml
task_model_mapping:
  complex_reasoning:
    primary: "openai_o3"
    fallback: ["claude"]
    confidence_threshold: 0.8

  code_generation:
    primary: "claude"
    specialists: ["openai_o3"]
    fallback: ["flash"]

  multimodal_analysis:
    primary: "gemini_pro"
    specialists: ["claude"]
    fallback: ["openai_o3"]

  quick_response:
    primary: "flash"
    fallback: ["claude"]
    timeout: 30
```

#### 动态权重调整
```yaml
dynamic_weights:
  enabled: true
  factors:
    performance_history: 0.4
    cost_efficiency: 0.3
    user_feedback: 0.3

  adjustment_frequency: "hourly"
  min_weight: 0.1
  max_weight: 0.5
```

### 7.2 协作模式

#### 主从模式
```yaml
master_slave_mode:
  master_model: "claude"
  slave_models: ["openai_o3", "gemini_pro"]

  workflow:
    - "master 分解任务"
    - "master 分配子任务"
    - "slaves 并行执行"
    - "master 整合结果"
    - "master 质量检查"
```

#### 对等模式
```yaml
peer_to_peer_mode:
  participating_models: ["claude", "gemini_pro", "openai_o3"]

  workflow:
    - "models 共同分析任务"
    - "分工协作处理"
    - "相互验证结果"
    - "集体决策最终输出"
```

#### 流水线模式
```yaml
pipeline_mode:
  stages:
    - name: "analysis"
      model: "claude"
      output: "analysis_result"
    - name: "processing"
      model: "openai_o3"
      input: "analysis_result"
      output: "processed_result"
    - name: "optimization"
      model: "gemini_pro"
      input: "processed_result"
      output: "final_result"
```

### 7.3 质量保证

#### 结果验证机制
```yaml
quality_assurance:
  cross_validation:
    enabled: true
    min_validators: 2
    consensus_threshold: 0.7

  consistency_check:
    enabled: true
    tolerance_level: 0.8

  feedback_loop:
    enabled: true
    learning_rate: 0.1
    adaptation_speed: "gradual"
```

#### 性能监控
```yaml
performance_monitoring:
  metrics:
    - "response_time"
    - "accuracy_score"
    - "cost_efficiency"
    - "user_satisfaction"

  thresholds:
    response_time:
      warning: 30
      critical: 60
    accuracy_score:
      warning: 0.8
      critical: 0.6
```

## 8. 故障排除

### 常见问题

#### 1. 服务器启动失败
```bash
# 检查配置文件
zen-mcp-server config validate

# 检查 API 密钥
zen-mcp-server keys test

# 查看错误日志
zen-mcp-server logs --level error

# 使用调试模式
zen-mcp-server --debug
```

#### 2. 模型连接问题
```bash
# 测试模型连接
zen-mcp-server test --model claude
zen-mcp-server test --model openai_o3
zen-mcp-server test --model gemini_pro

# 检查模型状态
zen-mcp-server models status

# 重新加载模型配置
zen-mcp-server models reload
```

#### 3. 协调失败
```bash
# 检查协调策略
zen-mcp-server strategy validate

# 查看任务执行日志
zen-mcp-server logs --filter orchestration

# 重置协调状态
zen-mcp-server orchestration reset
```

#### 4. 性能问题
```bash
# 检查性能指标
zen-mcp-server performance check

# 优化配置
zen-mcp-server optimize --config

# 清理缓存
zen-mcp-server cache clear
```

### 调试工具

#### 调试模式
```bash
# 启用详细日志
export ZEN_DEBUG=true
zen-mcp-server --debug

# 查看协调过程
zen-mcp-server debug --orchestration

# 模拟任务执行
zen-mcp-server simulate --task "coding_task" --models claude,openai_o3
```

#### 诊断工具
```bash
# 全面诊断
zen-mcp-server diagnose --full

# 检查特定组件
zen-mcp-server diagnose --component models
zen-mcp-server diagnose --component orchestration
zen-mcp-server diagnose --component coordination
```

### 性能优化

#### 配置优化
```yaml
optimization:
  enabled: true

  caching:
    strategy: "smart"
    ttl: 3600
    max_size: 1000

  connection_pooling:
    enabled: true
    max_connections: 20
    idle_timeout: 300

  request batching:
    enabled: true
    batch_size: 5
    max_wait_time: 1000
```

#### 监控和调优
```bash
# 性能基准测试
zen-mcp-server benchmark --models all

# 自动优化建议
zen-mcp-server optimize --suggest

# 应用优化建议
zen-mcp-server optimize --apply
```

## 更新日志

### v9.0.0 (最新)
- 🎉 新增 o3 模型支持
- ✨ 改进协调算法，提升性能 30%
- 🔧 增强质量保证机制
- 🐛 修复并发任务处理问题

### v8.2.0
- 🚀 新增自定义工作流支持
- ✨ 支持更多 AI 模型集成
- 🎨 改进 Web 管理界面
- 📊 增强实时监控功能

### v7.1.0
- 💾 新增智能缓存机制
- 🔍 改进模型选择算法
- ⚡ 优化响应速度
- 📚 完善文档和教程

### v6.1.0
- 🔄 重构核心架构
- ✨ 支持 Claude 指挥官模式
- 🎨 新增协作策略配置
- 🔧 增强安全功能

### v5.0.0
- 🎉 首个公开发布
- ⚡ 基础多模型协调功能
- 🔧 MCP 协议集成
- 📋 配置管理系统

## 贡献指南

欢迎为 Zen MCP Server 项目贡献代码！

### 开发环境
```bash
# 克隆仓库
git clone https://github.com/BeehiveInnovations/zen-mcp-server.git
cd zen-mcp-server

# 安装开发依赖
npm install

# 启动开发模式
npm run dev

# 运行测试
npm test

# 构建项目
npm run build
```

### 代码贡献
1. Fork 项目
2. 创建功能分支
3. 编写测试
4. 提交 Pull Request

### 模型集成
- 参考 [模型集成指南](https://github.com/BeehiveInnovations/zen-mcp-server/blob/main/docs/MODEL_INTEGRATION.md)
- 使用模型集成模板
- 遵循集成规范

## 许可证

本项目采用 Apache 2.0 许可证。详情请参阅 [LICENSE](https://github.com/BeehiveInnovations/zen-mcp-server/blob/main/LICENSE) 文件。

## 相关资源

- [GitHub 仓库](https://github.com/BeehiveInnovations/zen-mcp-server)
- [BeehiveInnovations 组织](https://github.com/BeehiveInnovations)
- [MCP 协议文档](https://modelcontextprotocol.io/)
- [Claude Code 文档](https://docs.anthropic.com/claude/docs/claude-code)
- [用户社区](https://github.com/BeehiveInnovations/zen-mcp-server/discussions)
- [Hacker News 讨论](https://news.ycombinator.com/item?id=44306798)

## 致谢

感谢以下项目和社区的支持：
- [Anthropic](https://www.anthropic.com/) - Claude 模型和 API
- [OpenAI](https://openai.com/) - o3 模型和 API
- [Google AI](https://ai.google.dev/) - Gemini 模型和 API
- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP 协议
- 开源社区的贡献者们

---

> 💡 **提示**: Zen MCP Server 是一个活跃开发的开源项目，已获得广泛关注（曾在 Hacker News 热议），如果你觉得有用，请考虑给项目一个 Star，或者参与贡献代码！让 AI 协作体验更加智能和高效！