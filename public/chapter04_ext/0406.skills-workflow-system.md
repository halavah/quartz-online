# Claude Skills Workflow System - Claude Skillså·¥ä½œæµç³»ç»Ÿ

> åŸºäºå®é™…ä½¿ç”¨ç»éªŒï¼Œä»‹ç»Claude Code Skillsç³»ç»Ÿçš„å¼€å‘å’Œä½¿ç”¨æ–¹æ³•ï¼Œæ„å»ºé«˜æ•ˆçš„AIå·¥ä½œæµ

## ğŸ“‹ ç›®å½•

1. [Skillsç³»ç»Ÿæ¦‚è¿°](#1-skillsç³»ç»Ÿæ¦‚è¿°)
2. [å†…ç½®Skillsä½¿ç”¨](#2-å†…ç½®skillsä½¿ç”¨)
3. [è‡ªå®šä¹‰Skillså¼€å‘](#3-è‡ªå®šä¹‰skillså¼€å‘)
4. [å·¥ä½œæµç¼–æ’](#4-å·¥ä½œæµç¼–æ’)
5. [æŠ€èƒ½é›†æˆå®æˆ˜](#5-æŠ€èƒ½é›†æˆå®æˆ˜)
6. [é«˜çº§åŠŸèƒ½åº”ç”¨](#6-é«˜çº§åŠŸèƒ½åº”ç”¨)
7. [æœ€ä½³å®è·µæ€»ç»“](#7-æœ€ä½³å®è·µæ€»ç»“)

## 1. Skillsç³»ç»Ÿæ¦‚è¿°

### Skillsæ¶æ„

Claude Code Skillsç³»ç»Ÿæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥ä½œæµç¼–æ’å¹³å°ï¼Œæ”¯æŒè‡ªå®šä¹‰æŠ€èƒ½å¼€å‘å’Œå¤æ‚ä»»åŠ¡è‡ªåŠ¨åŒ–ï¼š

```mermaid
graph TD
    A[Skills System] --> B[Core Engine]
    A --> C[Built-in Skills]
    A --> D[Custom Skills]
    A --> E[Workflow Orchestrator]

    B --> B1[Task Parser]
    B --> B2[Skill Router]
    B --> B3[State Manager]
    B --> B4[Result Aggregator]

    C --> C1[git]
    C --> C2[commit]
    C --> C3[pdf]
    C --> C4[review-pr]

    D --> D1[User Skills]
    D --> D2[Project Skills]
    D --> D3[Team Skills]

    E --> E1[Sequence Flow]
    E --> E2[Parallel Flow]
    E --> E3[Conditional Flow]
```

### æ ¸å¿ƒç‰¹æ€§

#### âœ¨ ä¸»è¦èƒ½åŠ›
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæŠ€èƒ½å¯ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
- **çµæ´»ç¼–æ’**ï¼šæ”¯æŒä¸²è¡Œã€å¹¶è¡Œã€æ¡ä»¶æ‰§è¡Œ
- **å‚æ•°ä¼ é€’**ï¼šæŠ€èƒ½é—´æ•°æ®æµè½¬å’ŒçŠ¶æ€å…±äº«
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **å¯è§†åŒ–ç®¡ç†**ï¼šå›¾å½¢åŒ–çš„å·¥ä½œæµç¼–è¾‘å™¨

#### ğŸ¯ åº”ç”¨åœºæ™¯
- **å¼€å‘è‡ªåŠ¨åŒ–**ï¼šCI/CDæµç¨‹ã€ä»£ç å®¡æŸ¥
- **æ–‡æ¡£å¤„ç†**ï¼šPDFè§£æã€æ ¼å¼è½¬æ¢
- **é¡¹ç›®ç®¡ç†**ï¼šä»»åŠ¡åˆ†é…ã€è¿›åº¦è·Ÿè¸ª
- **æ•°æ®å¤„ç†**ï¼šETLæµç¨‹ã€æŠ¥å‘Šç”Ÿæˆ

## 2. å†…ç½®Skillsä½¿ç”¨

### Gitç›¸å…³Skills

#### gitæŠ€èƒ½
```bash
# åŸºç¡€Gitæ“ä½œ
/skill git status
/skill git add .
/skill git commit -m "feat: add new feature"

# åˆ†æ”¯ç®¡ç†
/skill git checkout -b feature/new-feature
/skill git merge feature/new-feature
/skill git push origin main
```

#### commitæŠ€èƒ½
```bash
# æ™ºèƒ½æäº¤ç”Ÿæˆ
/skill commit

# å¸¦å‚æ•°çš„æäº¤
/skill commit --message "fix: resolve authentication issue"
/skill commit --add-only "*.js" --type "fix"
```

#### review-præŠ€èƒ½
```bash
# PRä»£ç å®¡æŸ¥
/skill review-pr 123
/skill review-pr --pr-number 123 --detailed
/skill review-pr --auto-comment --suggest-improvements
```

### æ–‡æ¡£å¤„ç†Skills

#### pdfæŠ€èƒ½
```bash
# PDFæ–‡æ¡£å¤„ç†
/skill pdf analyze document.pdf
/skill pdf extract-text document.pdf --output text.txt
/skill pdf convert document.pdf --format markdown
```

#### æ–‡ä»¶è½¬æ¢æŠ€èƒ½
```bash
# æ ¼å¼è½¬æ¢
/skill convert file.docx --to pdf
/skill convert data.json --to csv
/skill convert image.png --to webp --quality 80
```

### é¡¹ç›®ç®¡ç†Skills

#### ä»»åŠ¡ç®¡ç†
```bash
# ä»»åŠ¡åˆ†é…å’Œè·Ÿè¸ª
/skill assign-task --to "developer1" --task "Implement user auth"
/skill track-progress --project "website" --milestone "v1.0"
/skill generate-report --type "weekly" --format html
```

#### ä»£ç è´¨é‡æ£€æŸ¥
```bash
# ä»£ç è´¨é‡åˆ†æ
/skill analyze-code --path "src/" --metrics "complexity,coverage"
/skill find-bugs --threshold "high"
/skill suggest-refactoring --file "legacy.js"
```

## 3. è‡ªå®šä¹‰Skillså¼€å‘

### Skillå¼€å‘åŸºç¡€

#### Skillç»“æ„
```yaml
# skill-definition.yaml
name: "custom-analyzer"
version: "1.0.0"
description: "è‡ªå®šä¹‰æ•°æ®åˆ†ææŠ€èƒ½"
author: "Your Name"
tags: ["analysis", "data"]

# å‚æ•°å®šä¹‰
parameters:
  input_file:
    type: string
    required: true
    description: "è¾“å…¥æ–‡ä»¶è·¯å¾„"

  output_format:
    type: string
    default: "json"
    options: ["json", "csv", "markdown"]
    description: "è¾“å‡ºæ ¼å¼"

# æ‰§è¡Œæ­¥éª¤
steps:
  - name: "load_data"
    action: "read_file"
    params:
      file: "{{input_file}}"

  - name: "process_data"
    action: "run_script"
    script: |
      # æ•°æ®å¤„ç†é€»è¾‘
      data = load_data(input_file)
      processed = analyze_data(data)
      return processed

  - name: "save_result"
    action: "write_file"
    params:
      file: "output.{{output_format}}"
      content: "{{steps.process_data.result}}"
```

#### JavaScript Skillå¼€å‘
```javascript
// custom-skill.js
class CustomAnalyzer {
  constructor(config = {}) {
    this.config = {
      timeout: config.timeout || 30000,
      retries: config.retries || 3,
      ...config
    };
  }

  async execute(params) {
    try {
      // 1. å‚æ•°éªŒè¯
      this.validateParams(params);

      // 2. æ•°æ®åŠ è½½
      const data = await this.loadData(params.input_file);

      // 3. æ•°æ®å¤„ç†
      const result = await this.analyzeData(data, params);

      // 4. ç»“æœè¾“å‡º
      await this.saveResult(result, params.output_format);

      return {
        success: true,
        message: "åˆ†æå®Œæˆ",
        output: result
      };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        suggestion: this.getErrorMessage(error)
      };
    }
  }

  validateParams(params) {
    if (!params.input_file) {
      throw new Error("ç¼ºå°‘å¿…éœ€å‚æ•°: input_file");
    }

    if (!this.fileExists(params.input_file)) {
      throw new Error(`æ–‡ä»¶ä¸å­˜åœ¨: ${params.input_file}`);
    }
  }

  async loadData(filePath) {
    const fs = require('fs').promises;
    const content = await fs.readFile(filePath, 'utf8');

    // æ ¹æ®æ–‡ä»¶ç±»å‹è§£æ
    if (filePath.endsWith('.json')) {
      return JSON.parse(content);
    } else if (filePath.endsWith('.csv')) {
      return this.parseCSV(content);
    } else {
      return content;
    }
  }

  async analyzeData(data, params) {
    // è‡ªå®šä¹‰åˆ†æé€»è¾‘
    const analysis = {
      summary: this.generateSummary(data),
      insights: this.extractInsights(data),
      recommendations: this.generateRecommendations(data)
    };

    return analysis;
  }

  async saveResult(result, format) {
    const fs = require('fs').promises;
    let content;
    let filename;

    switch (format) {
      case 'json':
        content = JSON.stringify(result, null, 2);
        filename = 'analysis-result.json';
        break;
      case 'csv':
        content = this.convertToCSV(result);
        filename = 'analysis-result.csv';
        break;
      case 'markdown':
        content = this.convertToMarkdown(result);
        filename = 'analysis-result.md';
        break;
    }

    await fs.writeFile(filename, content);
    return filename;
  }
}

// æ³¨å†ŒæŠ€èƒ½
module.exports = {
  name: 'custom-analyzer',
  class: CustomAnalyzer,
  description: 'è‡ªå®šä¹‰æ•°æ®åˆ†ææŠ€èƒ½'
};
```

### Python Skillå¼€å‘

#### PythonæŠ€èƒ½æ¨¡æ¿
```python
# custom_skill.py
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, Optional

class DataAnalyzer:
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.required_params = ['input_file']

    def validate_params(self, params: Dict[str, Any]) -> None:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        for param in self.required_params:
            if param not in params:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")

        if not Path(params['input_file']).exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {params['input_file']}")

    def load_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        if file_path.endswith('.csv'):
            return pd.read_csv(file_path)
        elif file_path.endswith('.json'):
            return pd.read_json(file_path)
        elif file_path.endswith('.xlsx'):
            return pd.read_excel(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

    def analyze_data(self, data: pd.DataFrame, params: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®"""
        analysis = {
            'basic_stats': data.describe().to_dict(),
            'missing_values': data.isnull().sum().to_dict(),
            'data_types': data.dtypes.to_dict(),
            'shape': data.shape
        }

        # è‡ªå®šä¹‰åˆ†æé€»è¾‘
        if 'custom_analysis' in params:
            analysis['custom'] = self.custom_analysis(data, params['custom_analysis'])

        return analysis

    def custom_analysis(self, data: pd.DataFrame, config: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªå®šä¹‰åˆ†æé€»è¾‘"""
        results = {}

        for analysis_type, params in config.items():
            if analysis_type == 'correlation':
                results['correlation_matrix'] = data.corr().to_dict()
            elif analysis_type == 'groupby':
                group_col = params.get('column')
                if group_col in data.columns:
                    results['groupby_stats'] = data.groupby(group_col).describe().to_dict()

        return results

    def save_result(self, result: Dict[str, Any], output_format: str) -> str:
        """ä¿å­˜ç»“æœ"""
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')

        if output_format == 'json':
            filename = f"analysis_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

        elif output_format == 'csv':
            filename = f"analysis_{timestamp}.csv"
            # è½¬æ¢DataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(result)
            df.to_csv(filename, index=False)

        elif output_format == 'markdown':
            filename = f"analysis_{timestamp}.md"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("# æ•°æ®åˆ†ææŠ¥å‘Š\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {timestamp}\n\n")
                for key, value in result.items():
                    f.write(f"## {key}\n\n")
                    f.write(f"```json\n{json.dumps(value, ensure_ascii=False, indent=2)}\n```\n\n")

        return filename

# æŠ€èƒ½å…¥å£ç‚¹
def execute(params: Dict[str, Any]) -> Dict[str, Any]:
    """æŠ€èƒ½æ‰§è¡Œå…¥å£"""
    try:
        analyzer = DataAnalyzer()
        analyzer.validate_params(params)

        # åŠ è½½æ•°æ®
        data = analyzer.load_data(params['input_file'])

        # åˆ†ææ•°æ®
        result = analyzer.analyze_data(data, params)

        # ä¿å­˜ç»“æœ
        output_format = params.get('output_format', 'json')
        filename = analyzer.save_result(result, output_format)

        return {
            'success': True,
            'message': 'åˆ†æå®Œæˆ',
            'output_file': filename,
            'result': result
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'suggestion': 'è¯·æ£€æŸ¥è¾“å…¥å‚æ•°å’Œæ–‡ä»¶æ ¼å¼'
        }

# æŠ€èƒ½å…ƒæ•°æ®
skill_metadata = {
    'name': 'data-analyzer',
    'version': '1.0.0',
    'description': 'æ•°æ®åˆ†ææŠ€èƒ½ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å’Œåˆ†æç±»å‹',
    'author': 'Your Name',
    'parameters': {
        'input_file': {'type': 'string', 'required': True},
        'output_format': {'type': 'string', 'default': 'json', 'options': ['json', 'csv', 'markdown']},
        'custom_analysis': {'type': 'object', 'required': False}
    }
}
```

## 4. å·¥ä½œæµç¼–æ’

### å·¥ä½œæµå®šä¹‰

#### YAMLå·¥ä½œæµå®šä¹‰
```yaml
# workflow.yaml
name: "code-review-automation"
version: "1.0.0"
description: "è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å·¥ä½œæµ"

# å…¨å±€å˜é‡
variables:
  project_path: "./src"
  review_threshold: 80
  notification_channel: "#code-review"

# å·¥ä½œæµæ­¥éª¤
steps:
  - name: "load_changes"
    skill: "git"
    action: "diff"
    params:
      path: "{{project_path}}"
      target_branch: "main"
    output:
      changed_files: "files"

  - name: "analyze_code"
    skill: "custom-analyzer"
    params:
      input_files: "{{steps.load_changes.output.files}}"
      analysis_type: "quality,security,performance"
    parallel: true
    output:
      analysis_results: "results"

  - name: "check_tests"
    skill: "test-runner"
    params:
      files: "{{steps.load_changes.output.files}}"
      coverage_threshold: 80
    condition: "{{#contains steps.load_changes.output.files '*.test.js'}}"
    output:
      test_results: "tests"

  - name: "generate_report"
    skill: "report-generator"
    params:
      analysis: "{{steps.analyze_code.output.results}}"
      tests: "{{steps.check_tests.output.tests}}"
      template: "code-review-template"
    output:
      report_file: "report"

  - name: "notify_team"
    skill: "notification"
    params:
      channel: "{{notification_channel}}"
      message: "ä»£ç å®¡æŸ¥å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ: {{steps.generate_report.output.report_file}}"
      attachments: ["{{steps.generate_report.output.report_file}}"]
    on_error: "continue"

# é”™è¯¯å¤„ç†
error_handling:
  retry_count: 3
  retry_delay: 30
  notification_on_failure: true
```

#### æ¡ä»¶å·¥ä½œæµ
```yaml
# conditional-workflow.yaml
name: "deployment-pipeline"
description: "éƒ¨ç½²æµæ°´çº¿å·¥ä½œæµ"

steps:
  - name: "run_tests"
    skill: "test-runner"
    params:
      all_tests: true
      coverage: true
    output:
      success: "tests_passed"
      coverage: "test_coverage"

  - name: "security_scan"
    skill: "security-scanner"
    params:
      path: "./"
      severity_threshold: "medium"
    condition: "{{steps.run_tests.output.tests_passed}}"
    output:
      vulnerabilities: "security_issues"

  - name: "deploy_staging"
    skill: "deploy"
    params:
      environment: "staging"
      rollback_on_failure: true
    condition:
      and:
        - "{{steps.run_tests.output.tests_passed}}"
        - "{{lt steps.security_scan.output.vulnerabilities.high 0}}"
    output:
      deployment_url: "staging_url"

  - name: "integration_tests"
    skill: "integration-test"
    params:
      url: "{{steps.deploy_staging.output.staging_url}}"
      test_suite: "integration"
    condition: "{{steps.deploy_staging.output.staging_url}}"
    output:
      success: "integration_passed"

  - name: "deploy_production"
    skill: "deploy"
    params:
      environment: "production"
      approval_required: true
    condition: "{{steps.integration_tests.output.integration_passed}}"
    output:
      production_url: "prod_url"

  - name: "notify_success"
    skill: "notification"
    params:
      type: "success"
      message: "éƒ¨ç½²æˆåŠŸå®Œæˆ: {{steps.deploy_production.output.prod_url}}"
    condition: "{{steps.deploy_production.output.prod_url}}"
```

### å¹¶è¡Œå·¥ä½œæµ

#### å¹¶è¡Œä»»åŠ¡æ‰§è¡Œ
```mermaid
graph TD
    A[å¼€å§‹å·¥ä½œæµ] --> B[ä»£ç æ£€å‡º]
    B --> C[å¹¶è¡Œæ‰§è¡Œ]

    C --> D[å•å…ƒæµ‹è¯•]
    C --> E[ä»£ç åˆ†æ]
    C --> F[å®‰å…¨æ‰«æ]
    C --> G[ä¾èµ–æ£€æŸ¥]

    D --> H[ç­‰å¾…æ‰€æœ‰ä»»åŠ¡]
    E --> H
    F --> H
    G --> H

    H --> I{æ‰€æœ‰ä»»åŠ¡é€šè¿‡?}
    I -->|æ˜¯| J[æ„å»ºéƒ¨ç½²]
    I -->|å¦| K[å‘é€å¤±è´¥é€šçŸ¥]

    J --> L[éƒ¨ç½²æµ‹è¯•ç¯å¢ƒ]
    L --> M[é›†æˆæµ‹è¯•]
    M --> N[ç”Ÿäº§éƒ¨ç½²]

    K --> O[å·¥ä½œæµç»“æŸ]
    N --> O
```

#### å¹¶è¡Œå·¥ä½œæµå®šä¹‰
```yaml
# parallel-workflow.yaml
name: "comprehensive-analysis"
description: "å…¨é¢é¡¹ç›®åˆ†æå·¥ä½œæµ"

steps:
  - name: "setup"
    skill: "setup"
    params:
      project_path: "./"
    output:
      project_info: "info"

  - name: "parallel_analysis"
    parallel: true
    branches:
      - name: "code_quality"
        steps:
          - name: "lint_code"
            skill: "linter"
            params:
              path: "{{steps.setup.output.project_info.source_path}}"

          - name: "calculate_complexity"
            skill: "complexity-analyzer"
            params:
              files: "{{steps.setup.output.project_info.files}}"

      - name: "testing"
        steps:
          - name: "run_unit_tests"
            skill: "test-runner"
            params:
              type: "unit"
              coverage: true

          - name: "run_integration_tests"
            skill: "test-runner"
            params:
              type: "integration"

      - name: "security"
        steps:
          - name: "vulnerability_scan"
            skill: "security-scanner"
            params:
              severity: "high"

          - name: "dependency_check"
            skill: "dependency-checker"
            params:
              check_vulnerabilities: true

      - name: "performance"
        steps:
          - name: "benchmark"
            skill: "performance-test"
            params:
              scenarios: ["load", "stress"]

          - name: "profile_memory"
            skill: "memory-profiler"
            params:
              duration: 300

  - name: "aggregate_results"
    skill: "result-aggregator"
    params:
      inputs:
        - "{{steps.parallel_analysis.branches.code_quality}}"
        - "{{steps.parallel_analysis.branches.testing}}"
        - "{{steps.parallel_analysis.branches.security}}"
        - "{{steps.parallel_analysis.branches.performance}}"
    output:
      aggregated_report: "report"

  - name: "generate_dashboard"
    skill: "dashboard-generator"
    params:
      data: "{{steps.aggregate_results.output.aggregated_report}}"
      template: "project-dashboard"
    output:
      dashboard_url: "dashboard"
```

## 5. æŠ€èƒ½é›†æˆå®æˆ˜

### å®æˆ˜æ¡ˆä¾‹1ï¼šè‡ªåŠ¨åŒ–ä»£ç å‘å¸ƒ

#### å®Œæ•´å‘å¸ƒå·¥ä½œæµ
```yaml
# release-workflow.yaml
name: "automated-release"
description: "è‡ªåŠ¨åŒ–ä»£ç å‘å¸ƒæµç¨‹"

variables:
  version: ""
  release_notes: "CHANGELOG.md"
  deploy_envs: ["staging", "production"]

steps:
  - name: "pre_release_checks"
    skill: "pre-release-checker"
    params:
      checks: ["tests", "linting", "security", "documentation"]
    output:
      ready: "can_release"

  - name: "bump_version"
    skill: "version-bumper"
    params:
      type: "patch"  # auto-detect based on commits
      update_files: ["package.json", "version.txt"]
    condition: "{{steps.pre_release_checks.output.ready}}"
    output:
      new_version: "version"

  - name: "generate_changelog"
    skill: "changelog-generator"
    params:
      since: "{{variables.version}}"
      template: "standard"
      output_file: "{{variables.release_notes}}"
    output:
      changelog: "release_notes"

  - name: "create_git_tag"
    skill: "git"
    action: "tag"
    params:
      tag: "v{{steps.bump_version.output.new_version}}"
      message: "Release v{{steps.bump_version.output.new_version}}"

  - name: "build_artifacts"
    skill: "build"
    params:
      target: "production"
      optimize: true
      minify: true
    output:
      artifacts: "build_files"

  - name: "deploy_staging"
    skill: "deploy"
    params:
      environment: "staging"
      artifacts: "{{steps.build_artifacts.output.build_files}}"
      rollback: true
    output:
      staging_url: "staging_url"

  - name: "smoke_tests"
    skill: "smoke-test"
    params:
      url: "{{steps.deploy_staging.output.staging_url}}"
      critical_paths: ["/login", "/dashboard", "/api/health"]
    output:
      smoke_passed: "tests_ok"

  - name: "deploy_production"
    skill: "deploy"
    params:
      environment: "production"
      artifacts: "{{steps.build_artifacts.output.build_files}}"
      approval: true
    condition: "{{steps.smoke_tests.output.tests_ok}}"
    output:
      production_url: "prod_url"

  - name: "create_github_release"
    skill: "github"
    action: "create-release"
    params:
      tag: "v{{steps.bump_version.output.new_version}}"
      name: "Release v{{steps.bump_version.output.new_version}}"
      body: "{{steps.generate_changelog.output.release_notes}}"
      assets: "{{steps.build_artifacts.output.build_files}}"

  - name: "notify_teams"
    skill: "notification"
    params:
      channels: ["#releases", "#development"]
      message: |
        ğŸš€ Release v{{steps.bump_version.output.new_version}} deployed successfully!

        Staging: {{steps.deploy_staging.output.staging_url}}
        Production: {{steps.deploy_production.output.prod_url}}

        Release notes: {{steps.generate_changelog.output.release_notes}}
```

### å®æˆ˜æ¡ˆä¾‹2ï¼šæ™ºèƒ½æ•°æ®åˆ†æ

#### æ•°æ®åˆ†æå·¥ä½œæµ
```yaml
# data-analysis-workflow.yaml
name: "intelligent-data-analysis"
description: "æ™ºèƒ½æ•°æ®åˆ†æä¸æŠ¥å‘Šç”Ÿæˆ"

steps:
  - name: "data_ingestion"
    skill: "data-loader"
    params:
      sources:
        - type: "database"
          connection: "postgresql://..."
          query: "SELECT * FROM sales WHERE date >= '2024-01-01'"
        - type: "api"
          url: "https://api.external-data.com/metrics"
          auth: "bearer_token"
        - type: "file"
          path: "./data/supplementary_data.csv"
    output:
      raw_data: "dataset"

  - name: "data_cleaning"
    skill: "data-cleaner"
    params:
      data: "{{steps.data_ingestion.output.dataset}}"
      operations:
        - remove_duplicates
        - handle_missing_values
        - normalize_text
        - validate_data_types
    output:
      clean_data: "cleaned_dataset"

  - name: "exploratory_analysis"
    skill: "statistical-analyzer"
    params:
      data: "{{steps.data_cleaning.output.cleaned_dataset}}"
      analysis_types:
        - descriptive_stats
        - correlation_analysis
        - trend_analysis
        - outlier_detection
    output:
      insights: "statistical_insights"

  - name: "ml_analysis"
    skill: "machine-learning"
    params:
      data: "{{steps.data_cleaning.output.cleaned_dataset}}"
      models:
        - type: "clustering"
          algorithm: "kmeans"
          features: ["sales", "customers", "region"]
        - type: "forecasting"
          algorithm: "prophet"
          target: "revenue"
          period: "30d"
    output:
      predictions: "ml_results"

  - name: "visualization"
    skill: "chart-generator"
    params:
      data: "{{steps.data_cleaning.output.cleaned_dataset}}"
      insights: "{{steps.exploratory_analysis.output.statistical_insights}}"
      predictions: "{{steps.ml_analysis.output.ml_results}}"
      charts:
        - type: "line"
          title: "Sales Trend"
          x: "date"
          y: "revenue"
        - type: "heatmap"
          title: "Regional Performance"
          data: "correlation_matrix"
        - type: "scatter"
          title: "Customer Segments"
          x: "frequency"
          y: "value"
    output:
      charts: "visualizations"

  - name: "report_generation"
    skill: "report-generator"
    params:
      template: "executive-dashboard"
      data:
        summary: "{{steps.exploratory_analysis.output.statistical_insights}}"
        predictions: "{{steps.ml_analysis.output.ml_results}}"
        charts: "{{steps.visualization.output.visualizations}}"
      format: ["html", "pdf", "powerpoint"]
    output:
      reports: "analysis_reports"

  - name: "distribution"
    skill: "distributor"
    params:
      reports: "{{steps.report_generation.output.analysis_reports}}"
      channels:
        - type: "email"
          recipients: ["executives@company.com", "data-team@company.com"]
          subject: "Monthly Sales Analysis Report"
        - type: "slack"
          channels: ["#analytics", "#leadership"]
          message: "Monthly sales analysis report is ready!"
        - type: "dashboard"
          url: "https://dashboard.company.com/sales-analysis"
```

## 6. é«˜çº§åŠŸèƒ½åº”ç”¨

### æŠ€èƒ½ç‰ˆæœ¬ç®¡ç†

#### ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥
```yaml
# skill-versioning.yaml
skill: "data-analyzer"
version: "2.1.0"

# ç‰ˆæœ¬å†å²
versions:
  "2.1.0":
    changes:
      - "æ–°å¢æœºå™¨å­¦ä¹ æ¨¡å‹æ”¯æŒ"
      - "ä¼˜åŒ–å¤§æ•°æ®é›†å¤„ç†æ€§èƒ½"
      - "ä¿®å¤å†…å­˜æ³„æ¼é—®é¢˜"
    compatibility: "backward"
    migration_required: false

  "2.0.0":
    changes:
      - "é‡æ„æ ¸å¿ƒåˆ†æå¼•æ“"
      - "æ–°å¢å¹¶è¡Œå¤„ç†æ”¯æŒ"
      - "APIæ¥å£é‡å¤§æ›´æ–°"
    compatibility: "breaking"
    migration_required: true
    migration_script: "migrate_v2.py"

  "1.5.0":
    changes:
      - "å¢åŠ å¯è§†åŒ–æ”¯æŒ"
      - "ä¼˜åŒ–é”™è¯¯å¤„ç†"
    compatibility: "backward"
    migration_required: false

# ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ
compatibility_matrix:
  "2.1.0":
    supported_params:
      - "input_format"
      - "analysis_type"
      - "output_format"
      - "ml_model"
    deprecated_params:
      - "old_analysis_mode"
    removed_params:
      - "legacy_format"

# è‡ªåŠ¨è¿ç§»é…ç½®
migration:
  auto_migrate: true
  backup_before_migration: true
  test_migration: true
  rollback_on_failure: true
```

### æŠ€èƒ½å¸‚åœºä¸å…±äº«

#### æŠ€èƒ½å‘å¸ƒé…ç½®
```yaml
# skill-marketplace.yaml
skill_package:
  name: "advanced-data-processor"
  version: "1.2.0"
  author: "DataScience Team"
  license: "MIT"
  repository: "https://github.com/company/skills"

# å¸‚åœºå‘å¸ƒä¿¡æ¯
marketplace:
  category: "Data Processing"
  tags: ["data", "analysis", "ml", "automation"]
  description: "é«˜çº§æ•°æ®å¤„ç†å’Œåˆ†ææŠ€èƒ½ï¼Œæ”¯æŒæœºå™¨å­¦ä¹ é›†æˆ"

  screenshots:
    - "screenshots/overview.png"
    - "screenshots/configuration.png"
    - "screenshots/results.png"

  documentation:
    readme: "README.md"
    examples: "examples/"
    api_docs: "docs/api.md"

# ä¾èµ–ç®¡ç†
dependencies:
  skills:
    - name: "base-analyzer"
      version: ">=1.0.0"
    - name: "ml-integration"
      version: ">=2.0.0"

  python_packages:
    - "pandas>=1.3.0"
    - "scikit-learn>=1.0.0"
    - "matplotlib>=3.5.0"

  system_requirements:
    python: ">=3.8"
    memory: ">=2GB"
    disk_space: ">=1GB"

# æµ‹è¯•é…ç½®
testing:
  unit_tests: "tests/unit/"
  integration_tests: "tests/integration/"
  performance_tests: "tests/performance/"

  test_data:
    - "data/sample.csv"
    - "data/test_data.json"

  coverage_threshold: 85
  performance_benchmarks:
    processing_time: "<30s for 1MB data"
    memory_usage: "<500MB"
```

### ç›‘æ§å’Œæ—¥å¿—

#### æŠ€èƒ½ç›‘æ§é…ç½®
```yaml
# skill-monitoring.yaml
monitoring:
  metrics:
    execution_time:
      enabled: true
      alert_threshold: "5m"
      history_retention: "30d"

    success_rate:
      enabled: true
      alert_threshold: "<95%"
      window: "24h"

    error_rate:
      enabled: true
      alert_threshold: ">5%"
      window: "1h"

    resource_usage:
      cpu_usage:
        alert_threshold: ">80%"
      memory_usage:
        alert_threshold: ">1GB"
      disk_io:
        alert_threshold: ">100MB/s"

  logging:
    level: "INFO"
    format: "json"
    outputs:
      - type: "file"
        path: "/var/log/skills/{{skill_name}}.log"
        rotation: "daily"
        retention: "30d"
      - type: "elasticsearch"
        endpoint: "https://logs.company.com"
        index: "skills-logs"
      - type: "slack"
        webhook: "${SLACK_WEBHOOK_URL}"
        channel: "#skill-alerts"
        level: "ERROR"

  health_checks:
    endpoint: "/health"
    interval: "30s"
    timeout: "10s"
    retries: 3

    checks:
      - name: "database_connection"
        type: "external"
        config:
          url: "${DATABASE_URL}"
          timeout: "5s"

      - name: "api_availability"
        type: "http"
        config:
          url: "https://api.company.com/health"
          expected_status: 200
```

## 7. æœ€ä½³å®è·µæ€»ç»“

### å¼€å‘æœ€ä½³å®è·µ

#### æŠ€èƒ½è®¾è®¡åŸåˆ™
```yaml
# design-principles.yaml
principles:
  single_responsibility:
    description: "æ¯ä¸ªæŠ€èƒ½åªè´Ÿè´£ä¸€ä¸ªæ˜ç¡®çš„åŠŸèƒ½"
    example: "ä»£ç å®¡æŸ¥æŠ€èƒ½åªåšä»£ç åˆ†æï¼Œä¸åšéƒ¨ç½²"

  idempotent:
    description: "å¤šæ¬¡æ‰§è¡Œäº§ç”Ÿç›¸åŒç»“æœ"
    implementation: "ä½¿ç”¨å¹‚ç­‰æ€§è®¾è®¡å’Œå¹‚ç­‰æ€§æ£€æŸ¥"

  retryable:
    description: "æ”¯æŒå¤±è´¥é‡è¯•"
    implementation: "å®ç°å¹‚ç­‰æ€§å’ŒçŠ¶æ€æ¢å¤æœºåˆ¶"

  observable:
    description: "æä¾›å……åˆ†çš„æ—¥å¿—å’Œç›‘æ§"
    implementation: "ç»“æ„åŒ–æ—¥å¿—ã€æ€§èƒ½æŒ‡æ ‡ã€å¥åº·æ£€æŸ¥"

  testable:
    description: "æ˜“äºå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•"
    implementation: "ä¾èµ–æ³¨å…¥ã€æ¨¡æ‹Ÿæ¥å£ã€æµ‹è¯•æ•°æ®"
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€å·§
```yaml
# performance-optimization.yaml
optimization_strategies:
  caching:
    description: "ç¼“å­˜é‡å¤è®¡ç®—ç»“æœ"
    implementation:
      - "å†…å­˜ç¼“å­˜å¸¸ç”¨æ•°æ®"
      - "Redisç¼“å­˜åˆ†å¸ƒå¼ç»“æœ"
      - "æ–‡ä»¶ç¼“å­˜å¤§æ–‡ä»¶å¤„ç†ç»“æœ"

  parallel_processing:
    description: "å¹¶è¡Œå¤„ç†ç‹¬ç«‹ä»»åŠ¡"
    implementation:
      - "å¤šçº¿ç¨‹å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡"
      - "å¼‚æ­¥å¤„ç†I/Oå¯†é›†å‹ä»»åŠ¡"
      - "åˆ†å¸ƒå¼å¤„ç†å¤§è§„æ¨¡æ•°æ®"

  batch_processing:
    description: "æ‰¹é‡å¤„ç†å‡å°‘å¼€é”€"
    implementation:
      - "æ•°æ®åº“æ‰¹é‡æ“ä½œ"
      - "APIæ‰¹é‡è¯·æ±‚"
      - "æ–‡ä»¶æ‰¹é‡å¤„ç†"

  lazy_loading:
    description: "æŒ‰éœ€åŠ è½½èµ„æº"
    implementation:
      - "å»¶è¿Ÿåˆå§‹åŒ–é‡å¯¹è±¡"
      - "æŒ‰éœ€åŠ è½½é…ç½®æ–‡ä»¶"
      - "æµå¼å¤„ç†å¤§æ–‡ä»¶"
```

### å›¢é˜Ÿåä½œå»ºè®®

#### æŠ€èƒ½å…±äº«è§„èŒƒ
```markdown
# æŠ€èƒ½å…±äº«è§„èŒƒ

## æŠ€èƒ½å‘½åè§„èŒƒ
- ä½¿ç”¨æ¸…æ™°ã€æè¿°æ€§çš„åç§°
- é‡‡ç”¨åŠ¨è¯-åè¯ç»“æ„
- é¿å…ç¼©å†™å’Œä¸“ä¸šæœ¯è¯­

ç¤ºä¾‹ï¼š
- âœ… `analyze-code-quality`
- âœ… `generate-test-report`
- âŒ `acq` (acquireçš„ç¼©å†™)
- âŒ `data-proc-ml-v2`

## æ–‡æ¡£è¦æ±‚
æ¯ä¸ªæŠ€èƒ½å¿…é¡»åŒ…å«ï¼š
1. **README.md** - æŠ€èƒ½ä»‹ç»å’Œä½¿ç”¨è¯´æ˜
2. **CHANGELOG.md** - ç‰ˆæœ¬å˜æ›´è®°å½•
3. **examples/** - ä½¿ç”¨ç¤ºä¾‹
4. **tests/** - å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹

## ä»£ç è§„èŒƒ
- éµå¾ªé¡¹ç›®ç¼–ç æ ‡å‡†
- æ·»åŠ å……åˆ†çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- å®ç°é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- æä¾›æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯

## ç‰ˆæœ¬ç®¡ç†
- ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬ (SemVer)
- ä¸»ç‰ˆæœ¬å·å˜æ›´æ—¶æä¾›è¿ç§»æŒ‡å—
- ä¿æŒå‘åå…¼å®¹æ€§
```

### æ•…éšœæ’é™¤æŒ‡å—

#### å¸¸è§é—®é¢˜è§£å†³
```yaml
# troubleshooting-guide.yaml
common_issues:
  skill_not_found:
    symptoms:
      - "Error: Skill 'xxx' not found"
      - "æŠ€èƒ½æ‰§è¡Œå¤±è´¥"
    solutions:
      - æ£€æŸ¥æŠ€èƒ½åç§°æ‹¼å†™
      - ç¡®è®¤æŠ€èƒ½å·²æ­£ç¡®å®‰è£…
      - é‡æ–°åŠ è½½æŠ€èƒ½åº“

  parameter_validation_failed:
    symptoms:
      - "Parameter validation error"
      - "ç¼ºå°‘å¿…éœ€å‚æ•°"
    solutions:
      - æ£€æŸ¥å‚æ•°åç§°å’Œç±»å‹
      - æŸ¥çœ‹æŠ€èƒ½æ–‡æ¡£ç¡®è®¤å‚æ•°è¦æ±‚
      - ä½¿ç”¨é»˜è®¤å‚æ•°æµ‹è¯•

  execution_timeout:
    symptoms:
      - "Execution timeout"
      - "æŠ€èƒ½æ‰§è¡Œè¶…æ—¶"
    solutions:
      - å¢åŠ è¶…æ—¶æ—¶é—´é…ç½®
      - ä¼˜åŒ–æŠ€èƒ½æ€§èƒ½
      - æ£€æŸ¥ç½‘ç»œå’Œèµ„æºçŠ¶å†µ

  memory_exhausted:
    symptoms:
      - "Out of memory error"
      - "ç³»ç»Ÿå†…å­˜ä¸è¶³"
    solutions:
      - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
      - å¢åŠ ç³»ç»Ÿå†…å­˜
      - ä½¿ç”¨æµå¼å¤„ç†å¤§æ•°æ®

debugging_steps:
  1. "æ£€æŸ¥æŠ€èƒ½æ—¥å¿—"
  2. "éªŒè¯è¾“å…¥å‚æ•°"
  3. "æµ‹è¯•ç‹¬ç«‹åŠŸèƒ½"
  4. "æ£€æŸ¥èµ„æºä½¿ç”¨"
  5. "æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€"
```

## æ€»ç»“

Claude Skills Workflow System é€šè¿‡å¼ºå¤§çš„æŠ€èƒ½ç³»ç»Ÿå’Œå·¥ä½œæµç¼–æ’èƒ½åŠ›ï¼Œè®©å¤æ‚çš„è‡ªåŠ¨åŒ–ä»»åŠ¡å˜å¾—ç®€å•ï¼š

### ğŸ¯ æ ¸å¿ƒä»·å€¼

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæŠ€èƒ½å¯ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œç»´æŠ¤
2. **çµæ´»ç¼–æ’**ï¼šæ”¯æŒå¤æ‚çš„ä¸²è¡Œã€å¹¶è¡Œã€æ¡ä»¶å·¥ä½œæµ
3. **æ˜“äºæ‰©å±•**ï¼šå¼€æ”¾çš„æŠ€èƒ½å¼€å‘æ¡†æ¶å’Œä¸°å¯Œçš„API
4. **ä¼ä¸šçº§ç‰¹æ€§**ï¼šå®Œå–„çš„ç›‘æ§ã€æ—¥å¿—ã€ç‰ˆæœ¬ç®¡ç†

### ğŸš€ å®æ–½å»ºè®®

1. **ä»ç®€å•å¼€å§‹**ï¼šå…ˆä½¿ç”¨å†…ç½®æŠ€èƒ½ï¼Œå†å¼€å‘è‡ªå®šä¹‰æŠ€èƒ½
2. **é€æ­¥å®Œå–„**ï¼šæŒç»­ä¼˜åŒ–å·¥ä½œæµå’ŒæŠ€èƒ½æ€§èƒ½
3. **å›¢é˜Ÿåä½œ**ï¼šå»ºç«‹æŠ€èƒ½å…±äº«å’Œåä½œæœºåˆ¶
4. **æŒç»­ç›‘æ§**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»

é€šè¿‡åˆç†ä½¿ç”¨Skillsç³»ç»Ÿï¼Œå¯ä»¥å¤§å¹…æå‡å¼€å‘æ•ˆç‡ï¼Œå®ç°å¤æ‚çš„è‡ªåŠ¨åŒ–å·¥ä½œæµã€‚